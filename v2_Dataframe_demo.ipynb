{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-0-68.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdcc4901210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('demo').master(\"local\").enableHiveSupport().getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from the csv file and infering the schema\n",
    "df = spark.read.load(\"users.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the schema \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the elements of the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the schema instead of inferring it \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType, LongType\n",
    "\n",
    "fileSchema = StructType([StructField('name', StringType(),True),\n",
    "                        StructField('age', LongType(),True),\n",
    "                        StructField('job_title', StringType(),True)])\n",
    "\n",
    "df2 = spark.read.load(\"users.csv\", format=\"csv\", sep=\",\", schema = fileSchema, header=\"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing this dataframe in parquet\n",
    "df.write.parquet(\"users_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing this dataframe in json format\n",
    "df.write.json(\"users_2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing this dataframe in orc format\n",
    "df.write.orc(\"users_2.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from the JSON file and infering the schema\n",
    "df=spark.read.json(\"users_2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the schema in the case of json\n",
    "fileSchema = StructType([StructField('name', StringType(),True),\n",
    "                        StructField('age', IntegerType(),True),\n",
    "                        StructField('job', StringType(),True)])\n",
    "\n",
    "df2 = spark.read.json(\"users_2.json\", schema = fileSchema) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from the parquet file\n",
    "df = spark.read.parquet(\"users_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from the orc file\n",
    "df = spark.read.orc(\"users_2.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the Pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "#spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the pandas dataframe\n",
    "pdf = pd.DataFrame(np.random.rand(10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2\n",
      "0  0.941967  0.788411  0.647896\n",
      "1  0.301035  0.863868  0.765032\n",
      "2  0.824416  0.829005  0.118239\n",
      "3  0.938813  0.576990  0.257522\n",
      "4  0.000067  0.824416  0.633032\n",
      "5  0.452181  0.331384  0.361250\n",
      "6  0.297885  0.623081  0.734220\n",
      "7  0.380458  0.763270  0.699136\n",
      "8  0.659933  0.210418  0.696564\n",
      "9  0.892335  0.111163  0.936980\n"
     ]
    }
   ],
   "source": [
    "print(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the Data frames from the Pandas df\n",
    "#!pip install pyarrow\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: double (nullable = true)\n",
      " |-- 1: double (nullable = true)\n",
      " |-- 2: double (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+------------------+\n",
      "|                   0|                  1|                 2|\n",
      "+--------------------+-------------------+------------------+\n",
      "|   0.941967017531824| 0.7884106710442891|0.6478957837686703|\n",
      "| 0.30103509302473985| 0.8638681761162179|0.7650321588194177|\n",
      "|  0.8244161506430402| 0.8290051903795865|0.1182390449199483|\n",
      "|  0.9388129227481385| 0.5769901140456242|0.2575220496960423|\n",
      "|6.708929811183317E-5| 0.8244157959822621|0.6330319515096822|\n",
      "|  0.4521805781549154|0.33138388195168833|0.3612499875455727|\n",
      "|  0.2978852939161081| 0.6230806977080089|0.7342204127915506|\n",
      "| 0.38045792434330783| 0.7632701370534474|0.6991361824284357|\n",
      "|  0.6599326479266845|0.21041825867361974| 0.696564021682488|\n",
      "|  0.8923346720942692|0.11116324499733143|0.9369804834622025|\n",
      "+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2\n",
      "0  0.941967  0.788411  0.647896\n",
      "1  0.301035  0.863868  0.765032\n",
      "2  0.824416  0.829005  0.118239\n",
      "3  0.938813  0.576990  0.257522\n",
      "4  0.000067  0.824416  0.633032\n",
      "5  0.452181  0.331384  0.361250\n",
      "6  0.297885  0.623081  0.734220\n",
      "7  0.380458  0.763270  0.699136\n",
      "8  0.659933  0.210418  0.696564\n",
      "9  0.892335  0.111163  0.936980\n"
     ]
    }
   ],
   "source": [
    "# This need the install of PyArrow\n",
    "#   -- Installing step !pip install pyarrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "print(result_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######. operations on data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   name|age|       job|\n",
      "+-------+---+----------+\n",
      "| Vishwa| 61|  Engineer|\n",
      "|  Mohan| 79|    Doctor|\n",
      "|Rishavv| 21|   Student|\n",
      "|Shivani| 69|Consultant|\n",
      "| Sachin| 35| Cricketer|\n",
      "|  Rohit| 31|   Captain|\n",
      "|  Virat| 32|   Blogger|\n",
      "| Akshay| 45|     Actor|\n",
      "|Amitabh| 70| Superstar|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Select all columns in dataframe\n",
    "df = spark.read.orc(\"users_2.orc\")\n",
    "df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "| Vishwa|\n",
      "|  Mohan|\n",
      "|Rishavv|\n",
      "|Shivani|\n",
      "| Sachin|\n",
      "|  Rohit|\n",
      "|  Virat|\n",
      "| Akshay|\n",
      "|Amitabh|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Selecting specific column in dataframe\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name=u'Vishwa', age=61, job=u'Engineer'),\n",
       " Row(name=u'Mohan', age=79, job=u'Doctor'),\n",
       " Row(name=u'Shivani', age=69, job=u'Consultant'),\n",
       " Row(name=u'Amitabh', age=70, job=u'Superstar')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter operations\n",
    "df.filter(df['age']>50).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 31|    1|\n",
      "| 61|    1|\n",
      "| 35|    1|\n",
      "| 69|    1|\n",
      "| 45|    1|\n",
      "| 70|    1|\n",
      "| 21|    1|\n",
      "| 32|    1|\n",
      "| 79|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   name|age|       job|\n",
      "+-------+---+----------+\n",
      "|  Mohan| 79|    Doctor|\n",
      "|Amitabh| 70| Superstar|\n",
      "|Shivani| 69|Consultant|\n",
      "| Vishwa| 61|  Engineer|\n",
      "| Akshay| 45|     Actor|\n",
      "| Sachin| 35| Cricketer|\n",
      "|  Virat| 32|   Blogger|\n",
      "|  Rohit| 31|   Captain|\n",
      "|Rishavv| 21|   Student|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Order by\n",
    "df.orderBy(df.age.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################  Spark SQL   ##########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
